## Исходный код проверки доступности зеркал

1. в sstate.bbclass устанавливается переменная BB_HASHCHECK_FUNCTION = "sstate_checkhashes", в функции sstate_checkhashes есть множества found() и missed(), куда в последствии добавляются tid найденных и ненайденных задач соответственно. Составляется список задач tasklist, содержащий кортежи вида (tid, sstatefile). Выводится сообщение "Checking sstate mirror object availability" (которое мы можем увидеть в начале сборки), а затем для каждой задачи из tasklist вызывается метод checkstatus, определенный в этом же файле
2. В методе checkstatus для каждой задачи инициализируется объект класса Fetch, объявленного в https://github.com/yoctoproject/poky/blob/28fd497a26bdcc12d952f81436a6d873d81cd462/bitbake/lib/bb/fetch2/__init__.py. В конструкторе класса Fetch в поле self.ud для каждого url инициализируется объект класса FetchData, объявленного в этом же файле. Класс FetchData хранит состояние для заданного url. В конструкторе класса FetchData происходит итерирование по списку methods (объявленном в этом же файле), и для каждого выполняется проверка, поддерживает ли этот метод заданный url. После нахождения первого поддерживаемого метода цикл прекращается.
3. Объекты, хранящиеся в списке methods - это объекты классов-методов, являющихся наследниками (конкретными реализациями) класса FetchMethod, например класс Wget поддерживает протоколы 'http', 'https', 'ftp', 'ftps'.
4. Далее в sstate.bbclass вызывается метод checkstatus класса Fetch, который, в свою очредь, вызывает методы checkstatus и try_mirrors класса FetchMethod поочередно сначала для PREMIRRORS, потом для original uri, затем для MIRRORS. Метод try_mirrors класса FetchMethod вызывает функцию try_mirrors, которая строит uri для зеркала и вызывает функцию try_mirror_url, которая возвращает False, если необходимо попробовать другой url (то есть данный является нерабочим)


## Немного сведений о хэш-сервере
Когда bitbake просматривает входные данные для задачи сборки (рецепт, унаследованные
классы, соответствующие переменные, заданные в файлах conf, и входные данные других соответствующих
задач сборки), генерируется хэш входных данных. Это основа
кэширования общего состояния (sstate cache), предоставляемого bitbake - если в будущей сборке
хэш входных данных для задачи сборки будет таким же, ранее сгенерированное
sstate может быть повторно использовано вместо повторного запуска задачи сборки. Это позволяет
значительно сэкономить время при последующих сборках после
заполнения кэша sstate.

Хэш-сервер может улучшить работу sstate cache:
Он поддерживает базу данных входных хэшей для задач сборки и хэшей их выходных данных.
Традиционно, если входной хэш для задачи сборки изменяется, то все зависимые
задачи должны быть выполнены повторно, даже если выходные данные из первой задачи
идентичны. Однако при включенном хэш-сервере bitbake может обнаружить случай, когда
выходные данные задачи идентичны предыдущим выполнениям, и может отметить, что
два разных входных хэша эквивалентны. Это позволяет пропускать зависимые задачи, в которых доступны данные sstate для предыдущего входного хэша, что потенциально значительно улучшает повторное использование sstate и соответственно сокращает время сборки.

Таким образом, существует три типа хэшей:
1) taskhash: вычисляется на основе метаданных рецепта, кода задачи и значений хэша задачи из ее зависимостей. При внесении изменений эти хэши задачи изменяются, что приводит к повторному выполнению задачи. Таким образом, хэши задач, зависящих от этой задачи, также изменяются, что приводит к повторному выполнению всей цепочки зависимостей.

2) output hash: выходной хэш - вычисляется на основе выходных данных задач с общим состоянием, которые сохраняют свои выходные данные в хранилище sstate-cache. Сопоставление между хэшем задачи и ее выходным хэшем передается на хэш-сервер. Это сопоставление сохраняется сервером в базе данных для дальнейшего использования.

3) unihash: изначально присваивается хэшу задачи. Он используется для отслеживания уникальности выходных данных задачи. Когда включен хэш-сервер, bitbake вычисляет хэш задачи для каждой задачи, используя unihash ее зависимостей вместо taskhash задачи. Если входные данные для задачи поменяются, то изменится ее taskhash, следовательно, задачу придется запустить повторно, следовательно, ее output hash тоже будет пересчитан.После этого хэш-серверу будет передано сопоставление новых taskhash и output hash. Хэш-сервер сообщит bitbake'у, есть ли изменения в выходных данных, и, если нет, обновит значение unihash, чтобы он соответствовал исходному taskhash, который сгенерировал этот output hash. Благодаря этому зависимые задачи будут сохранять ранее записанный taskhash, и BitBake сможет извлекать их выходные данные из sstate-cache, вместо того чтобы повторно выполнять их. Аналогичным образом, выходные данные последующих задач, расположенных ниже по потоку, также могут быть получены из sstate-cache.

Для поиска файлов с кэшем в sstate-cache используются значения unihash.


## Общая схема того, что происходит
0. На IP:port, указанных в переменной BB_HASHSERVE_UPSTREAM, запускается клиент хэш-сервера, к которому в дальнейшем будет обращаться хэш-сервер. 
Это происходит в bitbake/lib/bb/cooker.py в функции handlePRServ. Происходит ping клиента, и, если клиент недоступен, то выводится предупреждение: WARNING: "BB_HASHSERVE_UPSTREAM is not valid, unable to connect hash equivalence server at ip:port". Там, где мы запустили хэш-сервер, генерируется файл hashserv.db, этот файл хранит маппинг output hash и taskhash. Благодаря этому маппингу, bitbake может узнавать о том, изменился ли output hash при изменении taskhash, то есть мы можем узнать о ситуации, когда входные данные изменились (например, добавился пробел), а выходные нет, в таком случае можем переиспользовать sstate-cache. (Если taskhash не менялся, то механизм HashEquivalence не нужен, bitbake и так найдет соответствующий файл).
1. В файле bitbake/lib/bb/runqueue.py в самом начале метода _execute_runqueue класса RunQueue вызывается метод класса RunQueueData prepare(). Данный метод выполняет подготовку к сборке, вот общий комментарий из исходного кода bitbake этого метода: "Turn a set of taskData into a RunQueue and compute data needed to optimise the execution order", то есть "Преобразует набор данных задач в очередь выполнения и вычисляет данные, необходимые для оптимизации порядка выполнения". По сути данный метод делает следующее: 
    1) Выполянется разрешение всех видов зависимостей, тем самым образуя очередь выполнения
    2) Заполняются данные о хэше задач, которые потом передаеются в sstate_checkhashes
            2.1. Данные о хэше заполняются следующим образом: вызывается код из bitbake/lib/bb/siggen.py, который выполняет генерацию хэша. В данном файле выполняется код в соответствии с заданным значением BB_SIGNATURE_HANDLER, то есть вызывается код соответствующего класса (там несколько реализаций BB_SIGNATURE_HANDLER).
                2.1.1. При заполнении происходит примерно следующее: сначала высчитываются taskhash для задач.
                2.1.2. Затем bitbake пытается взять значения unihash (это процесс более сложный). 
            2.2. В дальнейшем заполненные значения используются в sstate_checkhashes, когда генерируется имя sstate-файла (используется значение unihash).
2. В самом начале в функции sstate_checkhashes создаются множества found и missed. found - найденные в кэше задачи,  missed - остальные. В конце функции множество found возвращается как множество setscene задач (то есть тех, которые не нуждаются в перезапуске).
3. Внутри функции sstate_checkhashes происходит следующее:
    1) Переменная bitbake'a MIRRORS удаляется, а в переменную PREMIRRORS записываются SSTATE_MIRRORS, таким образом, на первом шаге, описанном в п.5, произойдет именно проверка SSTATE_MIRRORS, а дальше не будет лишних проверок, т.к. переменная MIRRORS удалена.
    2) создается список всех задач tasklist
    3) создается очередь соединений connection_cache_pool (объекты класса FetchConnectionCache). Длина очереди nproc устанавливается равной значению BB_NUMBER_THREADS (или, если длина tasklist меньше, чем BB_NUMBER_THREADS, тогда она устанавливается как длина tasklist). Это и будет число потоков, на которые далее распараллеливается проверка.
    4) Для каждой задачи при помощи использования класса ThreadPoolExecutor(max_workers=nproc) запускается проверка методом checkstatus (это уже происходит с распараллеливанием). То есть, когда заканчивается выполнение chechstatus для какой-то задачи, в очередь возвращается объект класса FetchConnectionCache. В это время ThreadPoolExecutor параллельно запускает проверки задач, число потоков=nproc. Например, при моей конфигурации это было 24. В данном случае connection_cache_pool это скорее вспомогательная информация, непосредственно распараллеливанием занимается объект класса ThreadPoolExecutor.
4. Внутри функции checkstatus происходит следующее:
    1) Для задачи создается объект класса Fetch (в его конструктор в качестве аргумента подается объект connection_cache, который берется как раз-таки из очереди connection_cahce_pool).
    2) вызывается его метод checkstatus.

5. Далее, если говорить обобщенно, происходит следующее: метод checkstatus класса Fetch ничего не возвращает, если находит соответствующий для задачи кэш. После такого хода выполнения метода checkstatus класса Fetch задача удаляется из множества missed и добавлятся в множество found. Иной вариант ход работы метода: если после всех проверок (PREMIRRORS, original uri, MIRRORS) метод так и не нашел соответствующего задаче кэша, то метод выбрасывает исключение FetchError, которое ловится в checkstatus внутри sstate_checkhashes, тем самым прерывая выполнение, из-за чего задача так и остается в множестве missed и не добавляется в множество found.
6. Разберем подробнее как происходит сама проверка:
    1) Внутри метода checkstatus класса Fetch сначала проверяются PREMIRRORS, затем, если там не найдено соответствующего кэша, вызывается проверка original uri, и затем, если и там ничего не найдено, вызывается проверка MIRRORS.
    2) Для PREMIRRORS и MIRRORS всё происходит аналогично: сначала берется соответствующая переменная  bitbake'а (PREMIRRORS или MIRRORS), эти "зеркала" передаются в метод try_mirros, где вызывается метод build_mirroruris, который возвращает два списка - список uris (адресов файла кэша для каждого проверяемого из зеркал), и список uds - (uri data's) - объектов класса FetchData, соответствующих каждому uri. Таким образом, эти два списка одной длины, и между их элементами есть соответствие. 
    Я проводила эксперимент с одним зеркалом, поднятым на localhost, и поэтому в моем случае из метода build_mirroruris возвращались всегда списки длины 1. Пример возвращаемого значения: `uris: ['http://127.0.0.1:8000/sstate-cache/37/5b/sstate%3Aattr%3Acore2-64-poky-linux%3A2.5.2%3Ar0%3Acore2-64%3A12%3A375b87b1b90a0f571357e6291be65aca51853aab8bca8eb02733ef029a95c85a_packagedata.tar.zst;downloadfilename=37/5b/sstate:attr:core2-64-poky-linux:2.5.2:r0:core2-64:12:375b87b1b90a0f571357e6291be65aca51853aab8bca8eb02733ef029a95c85a_packagedata.tar.zst'], uds: [<bb.fetch2.FetchData object at 0x7f3a644f4640>]`.  Далее финальный этап проверки: для каждого uri из списка uris вызывается метод try_mirror_url. Метод try_mirror_url в сценарии проверки выполняет лишь следующее: `found = ud.method.checkstatus(fetch, ud, ld)`. То есть вызывается метод checkstatus конкретного класса-метода (в моем случае, так как было поднято зеркло на localhost, это метод Local, но в случае по-другому устроенных зеркал, это будут методы соответствующих классов (все они являются наследниками FetchMethod)), а затем значение found возвращается из функции и далее вверх до метода try_mirrors. Если вернулось true - то проверка дальше не идет (например, после проверки PREMIRRORS вернулось true, значит далее проврека original uri и MIRRORS проводиться не будет). 
    3) Для original uri происходит примерно то же самое, за исключением того, что не берутся никакие переменные bitbake'a и не строятся uris. Просто вызывается метод checkstatus конкретного класса-метода и это значение возвращается в качестве результата. Аналогично, если здесь возвращается true, то проверка MIRRORS уже не происходит.
    4) В итоге, если после какой-либо из проверок: PREMIRRORS, original uri, MIRRORS вернулось true, то метод checkstatus класса Fetch завершает своё выполнение и ничего не возвращает. Иначе, если отовсюду вернулось false, выбрасывается исключение FetchError, и в функции sstate_checkhashes вызывается обработка исключения (пункт 4).

7. В результате мы получаем в множестве found необходимые setscene-задачи, в множестве missed - остальные.


Чтобы отключить механизм HashEquivalence, достаточно указать BB_SIGNATURE_HANDLER = "OEBasicHash", тогда задача будет взята из sstate-cache только в том случае, если ее входные данные не изменились (taskhash не изменился).

## Графическая схема
![image](https://github.com/user-attachments/assets/9b26b7b9-0415-4a96-85b8-af3dcab792b7)




## Вопросы к изучению
1) Как и когда заполняется hashserv.db


## Эксперимент по пересозданию хэш-сервера
Чтобы понять, каким образом информация о сопоставлении output hash и taskhash попадает на хэш-сервер, я провела эскперимент: 
я посмотрела сколько весит файл hashserv.db, который хранит информацию о данном маппинге, на моем хэш-сервере, который я использую, размер файла был 57,3кБ. Затем я удалила всю директорию, в которой запускала хэш-сервер, и пересоздала заново по инструкции из https://github.com/moevm/os_profiling/blob/main/wiki/yocto_cache/setup_OEEquivHash_server.md. После запуска команды bitbake-hashserv был вновь сгенерирован файл hashserv.db, который сразу после создания (до начала какой-либо сборки) весил 57,3кБ. Получается, что, вероятно, информацию о маппинге taskhash и output hash хэш-сервер получает не во время подготовки к сборке, а раньше.

## Ссылки на полезные источники
1. Очень хорошая презентация, объясняющая работу хэш-сервера: https://elinux.org/images/3/37/Hash_Equivalence_and_Reproducible_Builds.pdf
2. К презентации из п.1. видео: https://www.youtube.com/watch?v=zXEdqGS62Wc
3. Хорошая статья, объясняющая конфигурацию sstate и хэш-сервера: https://www.thegoodpenguin.co.uk/blog/improving-yocto-build-time/

